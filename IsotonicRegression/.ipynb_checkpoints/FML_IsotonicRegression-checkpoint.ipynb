{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evoR3TkHg1c8",
    "outputId": "82814111-9b5e-4b9b-f7e8-c0a71686f6b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-classification'...\n",
      "remote: Enumerating objects: 287, done.\u001b[K\n",
      "remote: Total 287 (delta 0), reused 0 (delta 0), pack-reused 287 (from 1)\u001b[K\n",
      "Receiving objects: 100% (287/287), 440.37 KiB | 27.52 MiB/s, done.\n",
      "Resolving deltas: 100% (167/167), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/bearpaw/pytorch-classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60Ublp-ugqg5",
    "outputId": "a17e5243-2843-4bd1-d05b-94d4807f81ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUTfO4wwgvqR"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/drive/Shareddrives/FML_Project/Project.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a499ec8",
    "outputId": "b9a8e56b-1bbb-4979-e75d-6014e9e531a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading and splitting CIFAR-10 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:14<00:00, 12.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split from original CIFAR-10 test set:\n",
      "  -> New Validation samples: 5000\n",
      "  -> New Test samples:       5000\n",
      "‚ùå Skipping local ResNet-164: No module named 'models'\n",
      "\n",
      "--- Running Isotonic Regression calibration for ResNet-56 (torch.hub) ---\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/releases/download/resnet/cifar10_resnet56-187c023a.pt\" to /root/.cache/torch/hub/checkpoints/cifar10_resnet56-187c023a.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.39M/3.39M [00:00<00:00, 56.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä Isotonic Regression Calibration for: ResNet-56 (Hub) (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Before Calibration ‚Üí Acc: 94.12%, Conf: 98.07%, ECE: 3.964%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-10/ResNet-56_(Hub)_C10_reliability_before_iso.png\n",
      "‚úÖ Isotonic Regression fitted on validation data\n",
      "\n",
      "After Calibration ‚Üí Acc: 94.12%, Conf: 94.53%, ECE: 0.841%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-10/ResNet-56_(Hub)_C10_reliability_after_iso.png\n",
      "\n",
      "==================================================================================================================================\n",
      "üìä Final Calibration Comparison on CIFAR-10 Test Set (Isotonic Regression)\n",
      "==================================================================================================================================\n",
      "Model                  |   Accuracy |  ECE (Before) |  ECE (After) |   Conf (Before) |    Conf (After)\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "ResNet-56 (Hub)        |     94.12% |       3.9644% |      0.8414% |          98.07% |          94.53%\n",
      "==================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.isotonic import IsotonicRegression # Import for Isotonic Regression\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalization stats for CIFAR-10\n",
    "transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# --- Load and Split Data (for CIFAR-10) ---\n",
    "print(\"\\nLoading and splitting CIFAR-10 data...\")\n",
    "try:\n",
    "    full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
    "    val_size, test_size = 5000, 5000\n",
    "    val_dataset, test_dataset = random_split(full_test_dataset, [val_size, test_size],\n",
    "                                             generator=torch.Generator().manual_seed(42))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
    "    print(f\"Data successfully split from original CIFAR-10 test set:\")\n",
    "    print(f\"  -> New Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"  -> New Test samples:       {len(test_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not load CIFAR-10 data. {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_conf, all_corr = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            all_conf.extend(conf.cpu().numpy())\n",
    "            all_corr.extend((pred == y).cpu().numpy())\n",
    "    return np.array(all_conf), np.array(all_corr)\n",
    "\n",
    "def calculate_ece(confidences, correct, n_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_lower, bin_upper = bin_boundaries[i], bin_boundaries[i + 1]\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(correct[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    return ece * 100\n",
    "\n",
    "def plot_reliability_diagram(confidences, correct, n_bins, model_name, suffix):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bin_boundaries[:-1], bin_boundaries[1:]\n",
    "    bin_accs, bin_confs, bin_props = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    for i, (bin_lower, bin_upper) in enumerate(zip(bin_lowers, bin_uppers)):\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        bin_props[i] = np.mean(in_bin)\n",
    "        if bin_props[i] > 0:\n",
    "            bin_accs[i] = np.mean(correct[in_bin])\n",
    "            bin_confs[i] = np.mean(confidences[in_bin])\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    plt.bar(bin_lowers + 1/(2*n_bins), bin_accs, width=1/n_bins*0.9, alpha=0.3, color='red', label='Accuracy')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    plt.xlabel('Confidence'); plt.ylabel('Accuracy'); plt.title(f'Reliability Diagram: {model_name} ({suffix})')\n",
    "    plt.legend(); plt.xlim(0, 1); plt.ylim(0, 1)\n",
    "    save_folder = \"/content/cifar-10\"; os.makedirs(save_folder, exist_ok=True)\n",
    "    filename = f\"{save_folder}/{model_name.replace(' ', '_')}_C10_reliability_{suffix}.png\"\n",
    "    plt.savefig(filename); plt.close()\n",
    "    print(f\"‚úÖ Saved reliability diagram: {filename}\")\n",
    "\n",
    "def load_checkpoint(model, path, device):\n",
    "    print(f\"Loading checkpoint: {path}\")\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    state_dict = ckpt.get('state_dict', ckpt)\n",
    "    new_sd = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())\n",
    "    model.load_state_dict(new_sd)\n",
    "    return model\n",
    "\n",
    "# ================================================\n",
    "# ISOTONIC REGRESSION CALIBRATION CLASS\n",
    "# ================================================\n",
    "class IsotonicRegressionCalibrator:\n",
    "    def __init__(self):\n",
    "        self.ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "\n",
    "    def fit(self, confidences, correct):\n",
    "        # We need probabilities to be sorted for IsotonicRegression to work correctly\n",
    "        # However, `fit` method of `IsotonicRegression` sorts internally based on `x` (confidences)\n",
    "        # So we just pass them as is.\n",
    "        self.ir.fit(confidences, correct)\n",
    "\n",
    "    def predict(self, confidences):\n",
    "        return self.ir.predict(confidences)\n",
    "\n",
    "# --- Main Calibration Function for Isotonic Regression ---\n",
    "def run_isotonic_calibration(model, model_name, n_bins=15):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üìä Isotonic Regression Calibration for: {model_name} (CIFAR-10)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # --- Predictions before calibration ---\n",
    "    conf_b, corr_b = get_predictions(model, test_loader, device)\n",
    "    acc_b = np.mean(corr_b) * 100\n",
    "    ece_b = calculate_ece(conf_b, corr_b, n_bins)\n",
    "    avg_conf_b = np.mean(conf_b) * 100\n",
    "\n",
    "    print(f\"\\nBefore Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_b:.2f}%, ECE: {ece_b:.3f}%\")\n",
    "    plot_reliability_diagram(conf_b, corr_b, n_bins, model_name, \"before_iso\")\n",
    "\n",
    "    # --- Fit Isotonic Regression on validation set ---\n",
    "    conf_val, corr_val = get_predictions(model, val_loader, device)\n",
    "    iso_reg = IsotonicRegressionCalibrator()\n",
    "    iso_reg.fit(conf_val, corr_val)\n",
    "    print(f\"‚úÖ Isotonic Regression fitted on validation data\")\n",
    "\n",
    "    # --- Apply on test set ---\n",
    "    calibrated_conf = iso_reg.predict(conf_b)\n",
    "    ece_a = calculate_ece(calibrated_conf, corr_b, n_bins)\n",
    "    avg_conf_a = np.mean(calibrated_conf) * 100\n",
    "\n",
    "    print(f\"\\nAfter Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_a:.2f}%, ECE: {ece_a:.3f}%\")\n",
    "    plot_reliability_diagram(calibrated_conf, corr_b, n_bins, model_name, \"after_iso\")\n",
    "\n",
    "    # --- Return result for table ---\n",
    "    return {\n",
    "        \"name\": model_name,\n",
    "        \"acc\": acc_b,\n",
    "        \"ece_before\": ece_b,\n",
    "        \"ece_after\": ece_a,\n",
    "        \"conf_before\": avg_conf_b,\n",
    "        \"conf_after\": avg_conf_a,\n",
    "    }\n",
    "\n",
    "# =============================================\n",
    "# --- RUN CALIBRATION FOR CIFAR-10 MODELS ---\n",
    "# =============================================\n",
    "all_results_iso = []\n",
    "\n",
    "try:\n",
    "    from models.cifar import resnet\n",
    "    print(\"\\n--- Running Isotonic Regression calibration for local ResNet-164 ---\")\n",
    "    model_resnet164 = resnet(depth=164, num_classes=10, block_name='Bottleneck')\n",
    "    path = '/content/Project/resnet110cifar10/model_best.pth.tar'\n",
    "    model_resnet164 = load_checkpoint(model_resnet164, path, device)\n",
    "    results = run_isotonic_calibration(model_resnet164, \"ResNet-164\")\n",
    "    all_results_iso.append(results)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Skipping local ResNet-164: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n--- Running Isotonic Regression calibration for ResNet-56 (torch.hub) ---\")\n",
    "    model_hub = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet56\", pretrained=True, trust_repo=True)\n",
    "    results = run_isotonic_calibration(model_hub, \"ResNet-56 (Hub)\")\n",
    "    all_results_iso.append(results)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Skipping ResNet-56 (Hub): {e}\")\n",
    "\n",
    "if all_results_iso:\n",
    "    print(\"\\n\" + \"=\"*130)\n",
    "    print(\"üìä Final Calibration Comparison on CIFAR-10 Test Set (Isotonic Regression)\")\n",
    "    print(\"=\"*130)\n",
    "    print(f\"{'Model':<22} | {'Accuracy':>10} | {'ECE (Before)':>13} | {'ECE (After)':>12} | {'Conf (Before)':>15} | {'Conf (After)':>15}\")\n",
    "    print(\"-\"*130)\n",
    "    for r in all_results_iso:\n",
    "        print(f\"{r['name']:<22} | {r['acc']:>9.2f}% | {r['ece_before']:>12.4f}% | {r['ece_after']:>11.4f}% | {r['conf_before']:>14.2f}% | {r['conf_after']:>14.2f}%\")\n",
    "    print(\"=\"*130)\n",
    "else:\n",
    "    print(\"\\nNo models were successfully calibrated with Isotonic Regression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "516e4bb7",
    "outputId": "63067802-5ccd-4e25-d29e-b41813415149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/pytorch-classification\n",
      "Using device: cuda\n",
      "\n",
      "Loading and splitting CIFAR-100 data for Isotonic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 169M/169M [00:13<00:00, 12.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 Data successfully split:\n",
      "  -> Validation samples: 5000\n",
      "  -> Test samples:       5000\n",
      "Successfully imported models from 'models.cifar' directory.\n",
      "Loading checkpoint: /content/Project/resnet164Cifar100/checkpoint.pth.tar\n",
      "\n",
      "======================================================================\n",
      "üìä Isotonic Regression Calibration for: ResNet-164 (CIFAR-100)\n",
      "======================================================================\n",
      "\n",
      "Before Calibration ‚Üí Acc: 73.14%, Conf: 88.21%, ECE: 15.075%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/ResNet-164_C100_reliability_before_iso_CIFAR-100.png\n",
      "‚úÖ Isotonic Regression fitted on validation data for CIFAR-100\n",
      "\n",
      "After Calibration ‚Üí Acc: 73.14%, Conf: 74.05%, ECE: 1.903%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/ResNet-164_C100_reliability_after_iso_CIFAR-100.png\n",
      "Loading checkpoint: /content/Project/densenet190Cifar100/checkpoint.pth.tar\n",
      "\n",
      "======================================================================\n",
      "üìä Isotonic Regression Calibration for: DenseNet-190 (CIFAR-100)\n",
      "======================================================================\n",
      "\n",
      "Before Calibration ‚Üí Acc: 82.26%, Conf: 89.55%, ECE: 7.335%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/DenseNet-190_C100_reliability_before_iso_CIFAR-100.png\n",
      "‚úÖ Isotonic Regression fitted on validation data for CIFAR-100\n",
      "\n",
      "After Calibration ‚Üí Acc: 82.26%, Conf: 82.92%, ECE: 1.614%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/DenseNet-190_C100_reliability_after_iso_CIFAR-100.png\n",
      "\n",
      "======================================================================\n",
      "üìä Isotonic Regression Calibration for: ResNet-56 (Hub) (CIFAR-100)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before Calibration ‚Üí Acc: 67.42%, Conf: 83.29%, ECE: 15.867%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/ResNet-56_(Hub)_C100_reliability_before_iso_CIFAR-100.png\n",
      "‚úÖ Isotonic Regression fitted on validation data for CIFAR-100\n",
      "\n",
      "After Calibration ‚Üí Acc: 67.42%, Conf: 67.20%, ECE: 1.941%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/ResNet-56_(Hub)_C100_reliability_after_iso_CIFAR-100.png\n",
      "Loading checkpoint: /content/Project/WRNCifar100/checkpoint.pth.tar\n",
      "\n",
      "======================================================================\n",
      "üìä Isotonic Regression Calibration for: WideResNet-28-10 (CIFAR-100)\n",
      "======================================================================\n",
      "\n",
      "Before Calibration ‚Üí Acc: 81.74%, Conf: 87.70%, ECE: 6.450%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/WideResNet-28-10_C100_reliability_before_iso_CIFAR-100.png\n",
      "‚úÖ Isotonic Regression fitted on validation data for CIFAR-100\n",
      "\n",
      "After Calibration ‚Üí Acc: 81.74%, Conf: 81.54%, ECE: 1.323%\n",
      "‚úÖ Saved reliability diagram: /content/cifar-100/WideResNet-28-10_C100_reliability_after_iso_CIFAR-100.png\n",
      "\n",
      "==================================================================================================================================\n",
      "üìä Final Isotonic Regression Comparison on CIFAR-100 Test Set\n",
      "==================================================================================================================================\n",
      "Model                  |   Accuracy |  ECE (Before) |  ECE (After) |   Conf (Before) |    Conf (After)\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "ResNet-164             |     73.14% |      15.0748% |      1.9028% |          88.21% |          74.05%\n",
      "DenseNet-190           |     82.26% |       7.3355% |      1.6138% |          89.55% |          82.92%\n",
      "ResNet-56 (Hub)        |     67.42% |      15.8666% |      1.9412% |          83.29% |          67.20%\n",
      "WideResNet-28-10       |     81.74% |       6.4495% |      1.3225% |          87.70% |          81.54%\n",
      "==================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# ISOTONIC REGRESSION CALIBRATION FOR CIFAR-100 MODELS\n",
    "# ========================================================\n",
    "%cd pytorch-classification/\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.isotonic import IsotonicRegression # Ensure this is imported\n",
    "\n",
    "# Define device (re-use from previous cells, ensuring it's available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- CIFAR-100 Data Loading and Transforms ---\n",
    "transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "print(\"\\nLoading and splitting CIFAR-100 data for Isotonic Regression...\")\n",
    "try:\n",
    "    full_test_dataset_cifar100 = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_cifar100)\n",
    "    val_size_cifar100 = 5000\n",
    "    test_size_cifar100 = len(full_test_dataset_cifar100) - val_size_cifar100\n",
    "\n",
    "    if test_size_cifar100 <= 0:\n",
    "        raise ValueError(\"Validation size exceeds CIFAR-100 test dataset size!\")\n",
    "\n",
    "    val_dataset_cifar100, test_dataset_cifar100 = random_split(\n",
    "        full_test_dataset_cifar100, [val_size_cifar100, test_size_cifar100],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    val_loader_cifar100 = DataLoader(val_dataset_cifar100, batch_size=100, shuffle=False, num_workers=2)\n",
    "    test_loader_cifar100 = DataLoader(test_dataset_cifar100, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"CIFAR-100 Data successfully split:\")\n",
    "    print(f\"  -> Validation samples: {len(val_dataset_cifar100)}\")\n",
    "    print(f\"  -> Test samples:       {len(test_dataset_cifar100)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not load CIFAR-100 data. {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Helper Functions (copied for self-containment) ---\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_conf, all_corr = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            all_conf.extend(conf.cpu().numpy())\n",
    "            all_corr.extend((pred == y).cpu().numpy())\n",
    "    return np.array(all_conf), np.array(all_corr)\n",
    "\n",
    "def calculate_ece(confidences, correct, n_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_lower, bin_upper = bin_boundaries[i], bin_boundaries[i + 1]\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(correct[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    return ece * 100\n",
    "\n",
    "def plot_reliability_diagram(confidences, correct, n_bins, model_name, suffix):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bin_boundaries[:-1], bin_boundaries[1:]\n",
    "    bin_accs, bin_confs, bin_props = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    for i, (bin_lower, bin_upper) in enumerate(zip(bin_lowers, bin_uppers)):\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        bin_props[i] = np.mean(in_bin)\n",
    "        if bin_props[i] > 0:\n",
    "            bin_accs[i] = np.mean(correct[in_bin])\n",
    "            bin_confs[i] = np.mean(confidences[in_bin])\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    bar_width = 1.0 / n_bins\n",
    "    bar_centers = bin_lowers + bar_width / 2\n",
    "    non_empty_mask = bin_props > 0\n",
    "    plt.bar(bar_centers[non_empty_mask], bin_accs[non_empty_mask], width=bar_width * 0.9, alpha=0.3, color='red', edgecolor='red', label='Accuracy')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    plt.xlabel('Confidence'); plt.ylabel('Accuracy'); plt.title(f'Reliability Diagram: {model_name} ({suffix})')\n",
    "    plt.legend(); plt.xlim(0, 1); plt.ylim(0, 1)\n",
    "    save_folder = \"/content/cifar-100\"; os.makedirs(save_folder, exist_ok=True) # Changed folder to cifar-100\n",
    "    filename = f\"{save_folder}/{model_name.replace(' ', '_')}_C100_reliability_{suffix}.png\" # Changed filename for cifar-100\n",
    "    plt.savefig(filename); plt.close()\n",
    "    print(f\"‚úÖ Saved reliability diagram: {filename}\")\n",
    "\n",
    "def load_checkpoint(model, path, device):\n",
    "    print(f\"Loading checkpoint: {path}\")\n",
    "    try:\n",
    "        ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint with weights_only=False: {e}\")\n",
    "        try:\n",
    "            print(\"Attempting fallback with weights_only=True...\")\n",
    "            ckpt = torch.load(path, map_location=device, weights_only=True)\n",
    "        except Exception as e_true:\n",
    "            print(f\"Fallback also failed: {e_true}\")\n",
    "            raise e\n",
    "\n",
    "    if 'state_dict' in ckpt:\n",
    "        state_dict = ckpt['state_dict']\n",
    "    else:\n",
    "        print(\"Warning: 'state_dict' key not found. Assuming checkpoint is the state_dict itself.\")\n",
    "        state_dict = ckpt\n",
    "\n",
    "    new_sd = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())\n",
    "    model.load_state_dict(new_sd)\n",
    "    return model\n",
    "\n",
    "# ================================================\n",
    "# ISOTONIC REGRESSION CALIBRATION CLASS\n",
    "# ================================================\n",
    "class IsotonicRegressionCalibrator:\n",
    "    def __init__(self):\n",
    "        self.ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "\n",
    "    def fit(self, confidences, correct):\n",
    "        self.ir.fit(confidences, correct)\n",
    "\n",
    "    def predict(self, confidences):\n",
    "        return self.ir.predict(confidences)\n",
    "\n",
    "# Re-define run_isotonic_calibration to accept specific loaders\n",
    "def run_isotonic_calibration_general(model, model_name, val_loader, test_loader, dataset_name, n_bins=15):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üìä Isotonic Regression Calibration for: {model_name} ({dataset_name})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # --- Predictions before calibration ---\n",
    "    conf_b, corr_b = get_predictions(model, test_loader, device)\n",
    "    acc_b = np.mean(corr_b) * 100\n",
    "    ece_b = calculate_ece(conf_b, corr_b, n_bins)\n",
    "    avg_conf_b = np.mean(conf_b) * 100\n",
    "\n",
    "    print(f\"\\nBefore Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_b:.2f}%, ECE: {ece_b:.3f}%\")\n",
    "    plot_reliability_diagram(conf_b, corr_b, n_bins, model_name, f\"before_iso_{dataset_name.replace(' ', '_')}\")\n",
    "\n",
    "    # --- Fit Isotonic Regression on validation set ---\n",
    "    conf_val, corr_val = get_predictions(model, val_loader, device)\n",
    "    iso_reg = IsotonicRegressionCalibrator()\n",
    "    iso_reg.fit(conf_val, corr_val)\n",
    "    print(f\"‚úÖ Isotonic Regression fitted on validation data for {dataset_name}\")\n",
    "\n",
    "    # --- Apply on test set ---\n",
    "    calibrated_conf = iso_reg.predict(conf_b)\n",
    "    ece_a = calculate_ece(calibrated_conf, corr_b, n_bins)\n",
    "    avg_conf_a = np.mean(calibrated_conf) * 100\n",
    "\n",
    "    print(f\"\\nAfter Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_a:.2f}%, ECE: {ece_a:.3f}%\")\n",
    "    plot_reliability_diagram(calibrated_conf, corr_b, n_bins, model_name, f\"after_iso_{dataset_name.replace(' ', '_')}\")\n",
    "\n",
    "    # --- Return result for table ---\n",
    "    return {\n",
    "        \"name\": model_name,\n",
    "        \"acc\": acc_b,\n",
    "        \"ece_before\": ece_b,\n",
    "        \"ece_after\": ece_a,\n",
    "        \"conf_before\": avg_conf_b,\n",
    "        \"conf_after\": avg_conf_a,\n",
    "    }\n",
    "\n",
    "all_results_cifar100_iso = []\n",
    "\n",
    "try:\n",
    "    from models.cifar import resnet\n",
    "    from models.cifar.densenet import densenet, Bottleneck\n",
    "    from models.cifar.wrn import WideResNet\n",
    "    print(\"Successfully imported models from 'models.cifar' directory.\")\n",
    "except ImportError:\n",
    "    print(\"WARNING: Could not import model definitions. Using mocks.\")\n",
    "    # Define simple MockModel for cases where actual models fail to import\n",
    "    class MockModel(nn.Module):\n",
    "        def __init__(self, num_classes=100):\n",
    "            super().__init__()\n",
    "            # Adjust input features for a common CIFAR-100 image size (3x32x32)\n",
    "            # A simple linear layer might not be suitable for actual image data\n",
    "            # but serves as a placeholder to prevent immediate crashes if imports fail.\n",
    "            self.fc = nn.Linear(32*32*3, num_classes) # Assuming flattened input for mock\n",
    "        def forward(self, x):\n",
    "            # Flatten the input tensor\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "\n",
    "    # Re-define functions to return MockModel with appropriate num_classes\n",
    "    def resnet_mock(depth, num_classes, block_name): return MockModel(num_classes=num_classes)\n",
    "    def densenet_mock(depth, num_classes, growthRate, compressionRate, block): return MockModel(num_classes=num_classes)\n",
    "    def wideresnet_mock(depth, num_classes, widen_factor, dropRate): return MockModel(num_classes=num_classes)\n",
    "\n",
    "    # Assign mocks to the original names\n",
    "    resnet = resnet_mock\n",
    "    densenet = densenet_mock\n",
    "    WideResNet = wideresnet_mock\n",
    "    Bottleneck = None # Bottleneck is a class used by densenet, can't mock easily without full definition\n",
    "\n",
    "try:\n",
    "    model_resnet164_cifar100 = resnet(depth=164, num_classes=100, block_name='Bottleneck')\n",
    "    path_resnet164 = '/content/Project/resnet164Cifar100/checkpoint.pth.tar'\n",
    "    model_resnet164_cifar100 = load_checkpoint(model_resnet164_cifar100, path_resnet164, device)\n",
    "    results = run_isotonic_calibration_general(model_resnet164_cifar100, \"ResNet-164\", val_loader_cifar100, test_loader_cifar100, \"CIFAR-100\")\n",
    "    all_results_cifar100_iso.append(results)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Skipping ResNet-164 (CIFAR-100): {e}\")\n",
    "\n",
    "try:\n",
    "    model_densenet_cifar100 = densenet(depth=190, num_classes=100, growthRate=40,\n",
    "                              compressionRate=2, block=Bottleneck)\n",
    "    path_densenet = '/content/Project/densenet190Cifar100/checkpoint.pth.tar'\n",
    "    model_densenet_cifar100 = load_checkpoint(model_densenet_cifar100, path_densenet, device)\n",
    "    results = run_isotonic_calibration_general(model_densenet_cifar100, \"DenseNet-190\", val_loader_cifar100, test_loader_cifar100, \"CIFAR-100\")\n",
    "    all_results_cifar100_iso.append(results)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Skipping DenseNet-190 (CIFAR-100): {e}\")\n",
    "\n",
    "try:\n",
    "    model_hub_cifar100 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\",\n",
    "                               \"cifar100_resnet56\", pretrained=True, trust_repo=True)\n",
    "    results = run_isotonic_calibration_general(model_hub_cifar100, \"ResNet-56 (Hub)\", val_loader_cifar100, test_loader_cifar100, \"CIFAR-100\")\n",
    "    all_results_cifar100_iso.append(results)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Skipping ResNet-56 (Hub) (CIFAR-100): {e}\")\n",
    "\n",
    "try:\n",
    "    model_wrn_cifar100 = WideResNet(depth=28, num_classes=100, widen_factor=10, dropRate=0.3)\n",
    "    path_wrn = '/content/Project/WRNCifar100/checkpoint.pth.tar'\n",
    "    model_wrn_cifar100 = load_checkpoint(model_wrn_cifar100, path_wrn, device)\n",
    "    results = run_isotonic_calibration_general(model_wrn_cifar100, \"WideResNet-28-10\", val_loader_cifar100, test_loader_cifar100, \"CIFAR-100\")\n",
    "    all_results_cifar100_iso.append(results)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Skipping WideResNet-28-10 (CIFAR-100): {e}\")\n",
    "\n",
    "\n",
    "if all_results_cifar100_iso:\n",
    "    print(\"\\n\" + \"=\"*130)\n",
    "    print(\"üìä Final Isotonic Regression Comparison on CIFAR-100 Test Set\")\n",
    "    print(\"=\"*130)\n",
    "    print(f\"{'Model':<22} | {'Accuracy':>10} | {'ECE (Before)':>13} | {'ECE (After)':>12} | {'Conf (Before)':>15} | {'Conf (After)':>15}\")\n",
    "    print(\"-\"*130)\n",
    "    for r in all_results_cifar100_iso:\n",
    "        print(f\"{r['name']:<22} | {r['acc']:>9.2f}% | {r['ece_before']:>12.4f}% | {r['ece_after']:>11.4f}% | {r['conf_before']:>14.2f}% | {r['conf_after']:>14.2f}%\")\n",
    "    print(\"=\"*130)\n",
    "else:\n",
    "    print(\"\\nNo CIFAR-100 models were successfully calibrated with Isotonic Regression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lg4sXN9XTHdX",
    "outputId": "9c45e74b-a32e-4bea-8a18-cf8d27388604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: inter-device move failed: '/kaggle/input/birds400/birds400' to '/content/birds400'; unable to remove target: Directory not empty\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"jutrera/stanford-car-dataset-by-classes-folder\")\n",
    "print(path)\n",
    "# Corrected: Move the entire 'car_data' directory, not its contents directly into /content/\n",
    "!mv {path}/* /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Hzv8GdFjgY-",
    "outputId": "c0169870-9ada-4164-d17e-78c44b9a2b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'birds400' dataset.\n",
      "mv: inter-device move failed: '/kaggle/input/birds400' to '/content/birds400'; unable to remove target: Directory not empty\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"antoniozarauzmoreno/birds400\")\n",
    "# Corrected: Move the entire 'birds400' directory, not its contents directly into /content/\n",
    "!mv {path} /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hn-lIY39lrL7",
    "outputId": "1847447c-f7c8-495c-cfdc-ecbcec907e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/pytorch-classification\n",
      "==> Using device: cuda\n",
      "\n",
      "Loading and splitting Stanford Cars data for Isotonic Regression...\n",
      "Found 196 classes in the Stanford Cars dataset.\n",
      "Stanford Cars Data successfully split:\n",
      "  -> Validation samples: 4000\n",
      "  -> Test samples:       4041\n",
      "\n",
      "Re-creating and loading MobileNetV2 model for Stanford Cars...\n",
      "Building MobileNetV2 for 196 classes.\n",
      "‚úÖ MobileNetV2 model loaded successfully.\n",
      "\n",
      "======================================================================\n",
      "üìä Isotonic Regression Calibration for: MobileNetV2 (Stanford Cars)\n",
      "======================================================================\n",
      "\n",
      "Before Calibration ‚Üí Acc: 44.35%, Conf: 53.77%, ECE: 9.421%\n",
      "‚úÖ Saved reliability diagram: /content/cars/MobileNetV2_reliability_before_iso_Stanford_Cars.png\n",
      "‚úÖ Isotonic Regression fitted on validation data for Stanford Cars\n",
      "\n",
      "After Calibration ‚Üí Acc: 44.35%, Conf: 44.13%, ECE: 1.525%\n",
      "‚úÖ Saved reliability diagram: /content/cars/MobileNetV2_reliability_after_iso_Stanford_Cars.png\n",
      "\n",
      "==================================================================================================================================\n",
      "üìä Final Isotonic Regression Comparison on Stanford Cars Test Set\n",
      "==================================================================================================================================\n",
      "Model                  |   Accuracy |  ECE (Before) |  ECE (After) |   Conf (Before) |    Conf (After)\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "MobileNetV2            |     44.35% |       9.4215% |      1.5248% |          53.77% |          44.13%\n",
      "==================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# ISOTONIC REGRESSION CALIBRATION FOR STANFORD CARS (MobileNetV2)\n",
    "# ========================================================\n",
    "%cd pytorch-classification/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from sklearn.isotonic import IsotonicRegression # Ensure this is imported\n",
    "\n",
    "# Define device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"==> Using device: {device}\")\n",
    "\n",
    "# --- Stanford Cars Data Loading and Transforms ---\n",
    "data_transforms_cars = {\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"\\nLoading and splitting Stanford Cars data for Isotonic Regression...\")\n",
    "try:\n",
    "    # Corrected path to point to the moved 'car_data' directory\n",
    "    full_test_dataset_cars = datasets.ImageFolder(\n",
    "        '/content/car_data/car_data/test',\n",
    "        data_transforms_cars['test']\n",
    "    )\n",
    "\n",
    "    num_classes_cars = len(full_test_dataset_cars.classes)\n",
    "    print(f\"Found {num_classes_cars} classes in the Stanford Cars dataset.\")\n",
    "\n",
    "    val_size_cars = 4000\n",
    "    test_size_cars = len(full_test_dataset_cars) - val_size_cars\n",
    "\n",
    "    if test_size_cars <= 0:\n",
    "        raise ValueError(\"Validation size exceeds Stanford Cars test dataset size!\")\n",
    "\n",
    "    val_dataset_cars, test_dataset_cars = random_split(\n",
    "        full_test_dataset_cars,\n",
    "        [val_size_cars, test_size_cars],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    val_loader_cars = DataLoader(val_dataset_cars, batch_size=32, shuffle=False, num_workers=2)\n",
    "    test_loader_cars = DataLoader(test_dataset_cars, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"Stanford Cars Data successfully split:\")\n",
    "    print(f\"  -> Validation samples: {len(val_dataset_cars)}\")\n",
    "    print(f\"  -> Test samples:       {len(test_dataset_cars)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading Stanford Cars data: {e}\")\n",
    "    # Exit gracefully if data loading fails to prevent subsequent errors\n",
    "    exit()\n",
    "\n",
    "# --- Helper Functions (copied for self-containment) ---\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_conf, all_corr = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            all_conf.extend(conf.cpu().numpy())\n",
    "            all_corr.extend((pred == y).cpu().numpy())\n",
    "    return np.array(all_conf), np.array(all_corr)\n",
    "\n",
    "def calculate_ece(confidences, correct, n_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_lower, bin_upper = bin_boundaries[i], bin_boundaries[i + 1]\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(correct[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    return ece * 100\n",
    "\n",
    "def plot_reliability_diagram(confidences, correct, n_bins, model_name, suffix):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bin_boundaries[:-1], bin_boundaries[1:]\n",
    "    bin_accs, bin_confs, bin_props = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    for i, (bin_lower, bin_upper) in enumerate(zip(bin_lowers, bin_uppers)):\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        bin_props[i] = np.mean(in_bin)\n",
    "        if bin_props[i] > 0:\n",
    "            bin_accs[i] = np.mean(correct[in_bin])\n",
    "            bin_confs[i] = np.mean(confidences[in_bin])\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    bar_width = 1.0 / n_bins\n",
    "    bar_centers = bin_lowers + bar_width / 2\n",
    "    non_empty_mask = bin_props > 0\n",
    "    plt.bar(bar_centers[non_empty_mask], bin_accs[non_empty_mask], width=bar_width * 0.9, alpha=0.3, color='red', edgecolor='red', label='Accuracy')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    plt.xlabel('Confidence'); plt.ylabel('Accuracy'); plt.title(f'Reliability Diagram: {model_name} ({suffix})')\n",
    "    plt.legend(); plt.xlim(0, 1); plt.ylim(0, 1)\n",
    "    save_folder = \"/content/cars\"; os.makedirs(save_folder, exist_ok=True)\n",
    "    filename = f\"{save_folder}/{model_name.replace(' ', '_')}_reliability_{suffix}.png\"\n",
    "    plt.savefig(filename); plt.close()\n",
    "    print(f\"‚úÖ Saved reliability diagram: {filename}\")\n",
    "\n",
    "def load_checkpoint(model, path, device):\n",
    "    print(f\"Loading checkpoint: {path}\")\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    state_dict = ckpt.get('state_dict', ckpt)\n",
    "    new_sd = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())\n",
    "    model.load_state_dict(new_sd)\n",
    "    return model\n",
    "\n",
    "# ================================================\n",
    "# ISOTONIC REGRESSION CALIBRATION CLASS\n",
    "# ================================================\n",
    "class IsotonicRegressionCalibrator:\n",
    "    def __init__(self):\n",
    "        self.ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "\n",
    "    def fit(self, confidences, correct):\n",
    "        self.ir.fit(confidences, correct)\n",
    "\n",
    "    def predict(self, confidences):\n",
    "        return self.ir.predict(confidences)\n",
    "\n",
    "# --- Main Calibration Function for Isotonic Regression ---\n",
    "def run_isotonic_calibration_general(model, model_name, val_loader, test_loader, dataset_name, n_bins=15):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üìä Isotonic Regression Calibration for: {model_name} ({dataset_name})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # --- Predictions before calibration ---\n",
    "    conf_b, corr_b = get_predictions(model, test_loader, device)\n",
    "    acc_b = np.mean(corr_b) * 100\n",
    "    ece_b = calculate_ece(conf_b, corr_b, n_bins)\n",
    "    avg_conf_b = np.mean(conf_b) * 100\n",
    "\n",
    "    print(f\"\\nBefore Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_b:.2f}%, ECE: {ece_b:.3f}%\")\n",
    "    plot_reliability_diagram(conf_b, corr_b, n_bins, model_name, f\"before_iso_{dataset_name.replace(' ', '_')}\")\n",
    "\n",
    "    # --- Fit Isotonic Regression on validation set ---\n",
    "    conf_val, corr_val = get_predictions(model, val_loader, device)\n",
    "    iso_reg = IsotonicRegressionCalibrator()\n",
    "    iso_reg.fit(conf_val, corr_val)\n",
    "    print(f\"‚úÖ Isotonic Regression fitted on validation data for {dataset_name}\")\n",
    "\n",
    "    # --- Apply on test set ---\n",
    "    calibrated_conf = iso_reg.predict(conf_b)\n",
    "    ece_a = calculate_ece(calibrated_conf, corr_b, n_bins)\n",
    "    avg_conf_a = np.mean(calibrated_conf) * 100\n",
    "\n",
    "    print(f\"\\nAfter Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_a:.2f}%, ECE: {ece_a:.3f}%\")\n",
    "    plot_reliability_diagram(calibrated_conf, corr_b, n_bins, model_name, f\"after_iso_{dataset_name.replace(' ', '_')}\")\n",
    "\n",
    "    # --- Return result for table ---\n",
    "    return {\n",
    "        \"name\": model_name,\n",
    "        \"acc\": acc_b,\n",
    "        \"ece_before\": ece_b,\n",
    "        \"ece_after\": ece_a,\n",
    "        \"conf_before\": avg_conf_b,\n",
    "        \"conf_after\": avg_conf_a,\n",
    "    }\n",
    "\n",
    "all_results_cars_iso = []\n",
    "\n",
    "try:\n",
    "    print(\"\\nRe-creating and loading MobileNetV2 model for Stanford Cars...\")\n",
    "    model_mobilenet_cars = models.mobilenet_v2(weights=None)\n",
    "\n",
    "    # num_classes_cars is defined within the try-except block for data loading\n",
    "    if 'num_classes_cars' not in locals():\n",
    "        raise NameError(\"num_classes_cars not defined. Data loading failed.\")\n",
    "\n",
    "    print(f\"Building MobileNetV2 for {num_classes_cars} classes.\")\n",
    "    in_features_mobilenet = model_mobilenet_cars.classifier[1].in_features\n",
    "    model_mobilenet_cars.classifier[1] = nn.Linear(in_features_mobilenet, num_classes_cars)\n",
    "    model_mobilenet_cars.to(device)\n",
    "\n",
    "    MODEL_PATH_MOBILENET = '/content/Project/MobilenetV2_Cars/model_best.pth'\n",
    "    model_mobilenet_cars.load_state_dict(torch.load(MODEL_PATH_MOBILENET, map_location=device))\n",
    "    print(\"‚úÖ MobileNetV2 model loaded successfully.\")\n",
    "\n",
    "    results = run_isotonic_calibration_general(model_mobilenet_cars, \"MobileNetV2\", val_loader_cars, test_loader_cars, \"Stanford Cars\")\n",
    "    all_results_cars_iso.append(results)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå CRITICAL ERROR: Could not find model for MobileNetV2 at '{MODEL_PATH_MOBILENET}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred with MobileNetV2 (Stanford Cars): {e}\")\n",
    "\n",
    "if all_results_cars_iso:\n",
    "    print(\"\\n\" + \"=\"*130)\n",
    "    print(\"üìä Final Isotonic Regression Comparison on Stanford Cars Test Set\")\n",
    "    print(\"=\"*130)\n",
    "    print(f\"{'Model':<22} | {'Accuracy':>10} | {'ECE (Before)':>13} | {'ECE (After)':>12} | {'Conf (Before)':>15} | {'Conf (After)':>15}\")\n",
    "    print(\"-\"*130)\n",
    "    for r in all_results_cars_iso:\n",
    "        print(f\"{r['name']:<22} | {r['acc']:>9.2f}% | {r['ece_before']:>12.4f}% | {r['ece_after']:>11.4f}% | {r['conf_before']:>14.2f}% | {r['conf_after']:>14.2f}%\")\n",
    "    print(\"=\"*130)\n",
    "else:\n",
    "    print(\"\\nNo Stanford Cars models were successfully calibrated with Isotonic Regression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJHNo7BJAIW0",
    "outputId": "12c510c7-4f37-4b08-f09a-ea599fc1df38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/pytorch-classification\n",
      "==> Using device: cuda\n",
      "\n",
      "Loading and splitting Birds Dataset data for Isotonic Regression...\n",
      "Found 400 classes in the Birds Dataset.\n",
      "Birds Dataset Data successfully split:\n",
      "  -> Validation samples: 1000\n",
      "  -> Test samples:       1000\n",
      "\n",
      "================================================================================\n",
      "STARTING ISOTONIC REGRESSION FOR: InceptionV3_Fold9 from /content/Project/InceptionNetV3_Birds/inceptionv3_birds9.pth\n",
      "================================================================================\n",
      "InceptionV3 model loaded successfully.\n",
      "\n",
      "======================================================================\n",
      "üìä Isotonic Regression Calibration for: InceptionV3_Fold9 (Birds Dataset)\n",
      "======================================================================\n",
      "\n",
      "Before Calibration ‚Üí Acc: 98.90%, Conf: 98.65%, ECE: 0.503%\n",
      "‚úÖ Saved reliability diagram: /content/birds/InceptionV3_Fold9_reliability_before_iso_Birds_Dataset.png\n",
      "‚úÖ Isotonic Regression fitted on validation data for Birds Dataset\n",
      "\n",
      "After Calibration ‚Üí Acc: 98.90%, Conf: 99.72%, ECE: 0.825%\n",
      "‚úÖ Saved reliability diagram: /content/birds/InceptionV3_Fold9_reliability_after_iso_Birds_Dataset.png\n",
      "\n",
      "==================================================================================================================================\n",
      "üìä Final Isotonic Regression Comparison on Birds Dataset Test Set\n",
      "==================================================================================================================================\n",
      "Model                  |   Accuracy |  ECE (Before) |  ECE (After) |   Conf (Before) |    Conf (After)\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "InceptionV3_Fold9      |     98.90% |       0.5027% |      0.8247% |          98.65% |          99.72%\n",
      "==================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# ISOTONIC REGRESSION CALIBRATION FOR BIRDS DATASET (InceptionV3)\n",
    "# ========================================================\n",
    "%cd pytorch-classification/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from sklearn.isotonic import IsotonicRegression # Ensure this is imported\n",
    "import torchvision # Needed for torch.serialization.add_safe_globals\n",
    "\n",
    "# Define device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"==> Using device: {device}\")\n",
    "\n",
    "# --- Birds Dataset Data Loading and Transforms ---\n",
    "data_transforms_birds = {\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"\\nLoading and splitting Birds Dataset data for Isotonic Regression...\")\n",
    "# Corrected path to point to the moved 'birds400' directory\n",
    "BIRDS_TEST_PATH = '/content/birds400/birds400/test'\n",
    "\n",
    "try:\n",
    "    full_test_dataset_birds = datasets.ImageFolder(BIRDS_TEST_PATH, data_transforms_birds['test'])\n",
    "    num_classes_birds = len(full_test_dataset_birds.classes)\n",
    "    print(f\"Found {num_classes_birds} classes in the Birds Dataset.\")\n",
    "\n",
    "    val_size_birds = 1000\n",
    "    test_size_birds = len(full_test_dataset_birds) - val_size_birds\n",
    "\n",
    "    if test_size_birds <= 0:\n",
    "        raise ValueError(\"Validation size exceeds Birds Dataset test dataset size!\")\n",
    "\n",
    "    val_dataset_birds, test_dataset_birds = random_split(full_test_dataset_birds, [val_size_birds, test_size_birds],\n",
    "                                             generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    val_loader_birds = DataLoader(val_dataset_birds, batch_size=32, shuffle=False, num_workers=2)\n",
    "    test_loader_birds = DataLoader(test_dataset_birds, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"Birds Dataset Data successfully split:\")\n",
    "    print(f\"  -> Validation samples: {len(val_dataset_birds)}\")\n",
    "    print(f\"  -> Test samples:       {len(test_dataset_birds)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: Birds Dataset directory not found at: {BIRDS_TEST_PATH}\")\n",
    "    print(\"Please make sure you ran the data preparation cell first.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred during Birds Dataset data loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Helper Functions (copied for self-containment) ---\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_conf, all_corr = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            if isinstance(logits, tuple):  # Handle InceptionV3 tuple output\n",
    "                logits = logits.logits\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            all_conf.extend(conf.cpu().numpy())\n",
    "            all_corr.extend((pred == labels).cpu().numpy())\n",
    "    return np.array(all_conf), np.array(all_corr)\n",
    "\n",
    "def calculate_ece(confidences, correct, n_bins=15):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_lower, bin_upper = bin_boundaries[i], bin_boundaries[i + 1]\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(correct[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    return ece * 100\n",
    "\n",
    "def plot_reliability_diagram(confidences, correct, n_bins, model_name, suffix):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bin_boundaries[:-1], bin_boundaries[1:]\n",
    "    bin_accs, bin_confs, bin_props = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    for i, (bin_lower, bin_upper) in enumerate(zip(bin_lowers, bin_uppers)):\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        bin_props[i] = np.mean(in_bin)\n",
    "        if bin_props[i] > 0:\n",
    "            bin_accs[i] = np.mean(correct[in_bin])\n",
    "            bin_confs[i] = np.mean(confidences[in_bin])\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    bar_width = 1.0 / n_bins\n",
    "    bar_centers = bin_lowers + bar_width / 2\n",
    "    non_empty_mask = bin_props > 0\n",
    "    plt.bar(bar_centers[non_empty_mask], bin_accs[non_empty_mask], width=bar_width * 0.9, alpha=0.3, color='red', edgecolor='red', label='Accuracy')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    plt.xlabel('Confidence'); plt.ylabel('Accuracy'); plt.title(f'Reliability Diagram: {model_name} ({suffix})')\n",
    "    plt.legend(); plt.xlim(0, 1); plt.ylim(0, 1)\n",
    "    save_folder = \"/content/birds\"; os.makedirs(save_folder, exist_ok=True)\n",
    "    filename = f\"{save_folder}/{model_name.replace(' ', '_')}_reliability_{suffix}.png\"\n",
    "    plt.savefig(filename); plt.close()\n",
    "    print(f\"‚úÖ Saved reliability diagram: {filename}\")\n",
    "\n",
    "def load_checkpoint(model, path, device):\n",
    "    print(f\"Loading checkpoint: {path}\")\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    state_dict = ckpt.get('state_dict', ckpt)\n",
    "    new_sd = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())\n",
    "    model.load_state_dict(new_sd)\n",
    "    return model\n",
    "\n",
    "# ================================================\n",
    "# ISOTONIC REGRESSION CALIBRATION CLASS\n",
    "# ================================================\n",
    "class IsotonicRegressionCalibrator:\n",
    "    def __init__(self):\n",
    "        self.ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "\n",
    "    def fit(self, confidences, correct):\n",
    "        self.ir.fit(confidences, correct)\n",
    "\n",
    "    def predict(self, confidences):\n",
    "        return self.ir.predict(confidences)\n",
    "\n",
    "# --- Main Calibration Function for Isotonic Regression ---\n",
    "def run_isotonic_calibration_general(model, model_name, val_loader, test_loader, dataset_name, n_bins=15):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üìä Isotonic Regression Calibration for: {model_name} ({dataset_name})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # --- Predictions before calibration ---\n",
    "    conf_b, corr_b = get_predictions(model, test_loader, device)\n",
    "    acc_b = np.mean(corr_b) * 100\n",
    "    ece_b = calculate_ece(conf_b, corr_b, n_bins)\n",
    "    avg_conf_b = np.mean(conf_b) * 100\n",
    "\n",
    "    print(f\"\\nBefore Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_b:.2f}%, ECE: {ece_b:.3f}%\")\n",
    "    plot_reliability_diagram(conf_b, corr_b, n_bins, model_name, f\"before_iso_{dataset_name.replace(' ', '_')}\")\n",
    "\n",
    "    # --- Fit Isotonic Regression on validation set ---\n",
    "    conf_val, corr_val = get_predictions(model, val_loader, device)\n",
    "    iso_reg = IsotonicRegressionCalibrator()\n",
    "    iso_reg.fit(conf_val, corr_val)\n",
    "    print(f\"‚úÖ Isotonic Regression fitted on validation data for {dataset_name}\")\n",
    "\n",
    "    # --- Apply on test set ---\n",
    "    calibrated_conf = iso_reg.predict(conf_b)\n",
    "    ece_a = calculate_ece(calibrated_conf, corr_b, n_bins)\n",
    "    avg_conf_a = np.mean(calibrated_conf) * 100\n",
    "\n",
    "    print(f\"\\nAfter Calibration ‚Üí Acc: {acc_b:.2f}%, Conf: {avg_conf_a:.2f}%, ECE: {ece_a:.3f}%\")\n",
    "    plot_reliability_diagram(calibrated_conf, corr_b, n_bins, model_name, f\"after_iso_{dataset_name.replace(' ', '_')}\")\n",
    "\n",
    "    # --- Return result for table ---\n",
    "    return {\n",
    "        \"name\": model_name,\n",
    "        \"acc\": acc_b,\n",
    "        \"ece_before\": ece_b,\n",
    "        \"ece_after\": ece_a,\n",
    "        \"conf_before\": avg_conf_b,\n",
    "        \"conf_after\": avg_conf_a,\n",
    "    }\n",
    "\n",
    "all_results_birds_iso = []\n",
    "\n",
    "try:\n",
    "    model_name_birds = \"InceptionV3_Fold9\"\n",
    "    MODEL_PATH_INCEPTION = \"/content/Project/InceptionNetV3_Birds/inceptionv3_birds9.pth\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"STARTING ISOTONIC REGRESSION FOR: {model_name_birds} from {MODEL_PATH_INCEPTION}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    torch.serialization.add_safe_globals([torchvision.models.inception.Inception3])\n",
    "    loaded_object_inception = torch.load(MODEL_PATH_INCEPTION, map_location=device, weights_only=False)\n",
    "\n",
    "    if isinstance(loaded_object_inception, nn.Module):\n",
    "        model_inception_birds = loaded_object_inception\n",
    "        in_features_inception = model_inception_birds.fc.in_features\n",
    "        if model_inception_birds.fc.out_features != num_classes_birds:\n",
    "            model_inception_birds.fc = nn.Linear(in_features_inception, num_classes_birds)\n",
    "        model_inception_birds.to(device)\n",
    "    elif isinstance(loaded_object_inception, dict):\n",
    "        model_inception_birds = models.inception_v3(weights=None, aux_logits=False, init_weights=False)\n",
    "        in_features_inception = model_inception_birds.fc.in_features\n",
    "        model_inception_birds.fc = nn.Linear(in_features_inception, num_classes_birds)\n",
    "        model_inception_birds.to(device)\n",
    "        state_dict_inception = loaded_object_inception.get('state_dict', loaded_object_inception)\n",
    "        new_sd_inception = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict_inception.items())\n",
    "        model_inception_birds.load_state_dict(new_sd_inception)\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected object type for InceptionV3: {type(loaded_object_inception)}\")\n",
    "\n",
    "    print(\"InceptionV3 model loaded successfully.\")\n",
    "    results = run_isotonic_calibration_general(model_inception_birds, model_name_birds, val_loader_birds, test_loader_birds, \"Birds Dataset\")\n",
    "    all_results_birds_iso.append(results)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå CRITICAL ERROR: Could not find weights for InceptionV3 at '{MODEL_PATH_INCEPTION}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred with InceptionV3 (Birds Dataset): {e}\")\n",
    "\n",
    "if all_results_birds_iso:\n",
    "    print(\"\\n\" + \"=\"*130)\n",
    "    print(\"üìä Final Isotonic Regression Comparison on Birds Dataset Test Set\")\n",
    "    print(\"=\"*130)\n",
    "    print(f\"{'Model':<22} | {'Accuracy':>10} | {'ECE (Before)':>13} | {'ECE (After)':>12} | {'Conf (Before)':>15} | {'Conf (After)':>15}\")\n",
    "    print(\"-\"*130)\n",
    "    for r in all_results_birds_iso:\n",
    "        print(f\"{r['name']:<22} | {r['acc']:>9.2f}% | {r['ece_before']:>12.4f}% | {r['ece_after']:>11.4f}% | {r['conf_before']:>14.2f}% | {r['conf_after']:>14.2f}%\")\n",
    "    print(\"=\"*130)\n",
    "else:\n",
    "    print(\"\\nNo Birds Dataset models were successfully calibrated with Isotonic Regression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4kIRKJvoYe5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
